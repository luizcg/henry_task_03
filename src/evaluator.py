"""
Evaluator module (Bonus).

Evaluates the quality of responses generated by the multi-agent system.
Scores each response on a 1-10 scale and reports scores to Langfuse.

Evaluation dimensions:
- Relevance: Does the answer address the user's question?
- Completeness: Is the answer thorough and comprehensive?
- Accuracy: Is the information correct based on the provided context?
"""

import json
import logging
import os
from typing import Optional

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import Runnable, RunnableLambda

from src.config import LLM_MODEL_NAME

logger = logging.getLogger(__name__)


# =============================================================================
# Evaluation Chain
# =============================================================================

def get_evaluator_chain() -> Runnable:
    """
    Create the evaluator chain for scoring responses.

    The evaluator takes:
    - query: Original user question
    - answer: Generated answer
    - context: Retrieved context used to generate the answer

    Returns:
    - score: Integer 1-10
    - relevance: Score for relevance (1-10)
    - completeness: Score for completeness (1-10)
    - accuracy: Score for accuracy (1-10)
    - justification: Brief explanation of the scores
    """
    llm = ChatOpenAI(
        model=LLM_MODEL_NAME,
        temperature=0,
    )

    system_prompt = """You are a quality evaluator for an enterprise support system.
Your job is to evaluate the quality of answers generated by AI assistants.

Evaluate each answer on three dimensions (1-10 scale):

1. **Relevance** (1-10): Does the answer directly address the user's question?
   - 1-3: Answer is off-topic or doesn't address the question
   - 4-6: Answer partially addresses the question
   - 7-10: Answer directly and fully addresses the question

2. **Completeness** (1-10): Is the answer thorough?
   - 1-3: Answer is missing critical information
   - 4-6: Answer covers basics but lacks depth
   - 7-10: Answer is comprehensive and thorough

3. **Accuracy** (1-10): Is the information correct based on the context provided?
   - 1-3: Contains significant errors or unsupported claims
   - 4-6: Mostly accurate with minor issues
   - 7-10: Accurate and well-supported by the context

The overall score should be the average of the three dimensions, rounded to nearest integer.

Respond with valid JSON only:
{{
  "relevance": <int>,
  "completeness": <int>,
  "accuracy": <int>,
  "score": <int>,
  "justification": "<brief explanation>"
}}"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", """Please evaluate this response:

**User Question:**
{query}

**Context Provided:**
{context}

**Generated Answer:**
{answer}

Provide your evaluation as JSON."""),
    ])

    def parse_evaluation(response: str) -> dict:
        """Parse the evaluation response."""
        try:
            # Clean up response
            cleaned = response.strip()
            if cleaned.startswith("```"):
                cleaned = cleaned.split("```")[1]
                if cleaned.startswith("json"):
                    cleaned = cleaned[4:]
            cleaned = cleaned.strip()

            result = json.loads(cleaned)

            # Validate and normalize scores
            for key in ["relevance", "completeness", "accuracy", "score"]:
                if key in result:
                    result[key] = max(1, min(10, int(result[key])))

            # Calculate overall score if not provided
            if "score" not in result:
                scores = [result.get(k, 5) for k in ["relevance", "completeness", "accuracy"]]
                result["score"] = round(sum(scores) / len(scores))

            return result

        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.warning(f"Failed to parse evaluation: {e}")
            return {
                "relevance": 5,
                "completeness": 5,
                "accuracy": 5,
                "score": 5,
                "justification": "Failed to parse evaluation",
            }

    chain = prompt | llm | StrOutputParser() | RunnableLambda(parse_evaluation)

    return chain


def evaluate_response(
    query: str,
    answer: str,
    context: str = "",
) -> dict:
    """
    Evaluate a response's quality.

    Args:
        query: Original user question.
        answer: Generated answer.
        context: Retrieved context (optional).

    Returns:
        Dict with scores and justification.
    """
    evaluator = get_evaluator_chain()

    result = evaluator.invoke({
        "query": query,
        "answer": answer,
        "context": context or "No context provided",
    })

    return result


# =============================================================================
# Langfuse Score Reporting
# =============================================================================

def report_score_to_langfuse(
    trace_id: str,
    evaluation: dict,
    observation_id: Optional[str] = None,
) -> bool:
    """
    Report evaluation scores to Langfuse.

    Args:
        trace_id: Langfuse trace ID.
        evaluation: Dict with score, relevance, completeness, accuracy, justification.
        observation_id: Optional specific observation ID to score.

    Returns:
        True if successful, False otherwise.
    """
    try:
        from langfuse import Langfuse

        langfuse = Langfuse()

        # Report overall score
        langfuse.score(
            trace_id=trace_id,
            observation_id=observation_id,
            name="answer_quality",
            value=evaluation.get("score", 5),
            comment=evaluation.get("justification", ""),
        )

        # Report individual dimension scores
        for dimension in ["relevance", "completeness", "accuracy"]:
            if dimension in evaluation:
                langfuse.score(
                    trace_id=trace_id,
                    observation_id=observation_id,
                    name=f"answer_{dimension}",
                    value=evaluation[dimension],
                )

        logger.info(f"Reported scores to Langfuse for trace {trace_id}")
        return True

    except ImportError:
        logger.warning("Langfuse not installed, skipping score reporting")
        return False
    except Exception as e:
        logger.error(f"Failed to report scores to Langfuse: {e}")
        return False


# =============================================================================
# Evaluation Pipeline
# =============================================================================

def evaluate_and_report(
    query: str,
    answer: str,
    context: str = "",
    trace_id: Optional[str] = None,
) -> dict:
    """
    Full evaluation pipeline: evaluate response and optionally report to Langfuse.

    Args:
        query: Original user question.
        answer: Generated answer.
        context: Retrieved context.
        trace_id: Langfuse trace ID (if available).

    Returns:
        Evaluation results dict.
    """
    # Run evaluation
    evaluation = evaluate_response(query, answer, context)

    logger.info(
        f"Evaluation: score={evaluation.get('score')}, "
        f"relevance={evaluation.get('relevance')}, "
        f"completeness={evaluation.get('completeness')}, "
        f"accuracy={evaluation.get('accuracy')}"
    )

    # Report to Langfuse if trace_id provided
    if trace_id:
        report_score_to_langfuse(trace_id, evaluation)

    return evaluation


# =============================================================================
# CLI for Testing
# =============================================================================

def main():
    """
    CLI for the evaluator.

    Supports two modes:
    1. Piped JSON from assistant: acme-assistant -q "..." --json | acme-evaluate
    2. Manual arguments: acme-evaluate -q "..." -a "..." -c "..."
    """
    import argparse
    import sys

    parser = argparse.ArgumentParser(
        description="Evaluate assistant responses",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Pipe from assistant (recommended)
    acme-assistant -q "How do I request PTO?" --json | acme-evaluate

    # Manual evaluation
    acme-evaluate -q "How do I request PTO?" -a "Submit via HR Portal..." -c "PTO policy..."
        """,
    )
    parser.add_argument("--query", "-q", help="User question (optional if piped)")
    parser.add_argument("--answer", "-a", help="Generated answer (optional if piped)")
    parser.add_argument("--context", "-c", default="", help="Retrieved context")

    args = parser.parse_args()

    from dotenv import load_dotenv
    load_dotenv()

    query = args.query
    answer = args.answer
    context = args.context

    # Check for piped input
    if not sys.stdin.isatty():
        stdin_data = sys.stdin.read().strip()
        if stdin_data:
            try:
                data = json.loads(stdin_data)
                query = data.get("query", query)
                answer = data.get("final_answer", answer)
                # Build context from sources if available
                sources = data.get("sources", [])
                context = data.get("context", "") or "\n".join(sources)
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse piped JSON: {e}")
                print(f"‚ùå Error: Invalid JSON input. Use --json flag with assistant.")
                sys.exit(1)

    # Validate required fields
    if not query:
        print("‚ùå Error: Missing query. Use -q or pipe JSON from assistant.")
        sys.exit(1)
    if not answer:
        print("‚ùå Error: Missing answer. Use -a or pipe JSON from assistant.")
        sys.exit(1)

    result = evaluate_response(query, answer, context)

    print("\nüìä Evaluation Results:")
    print(f"   Overall Score: {result.get('score')}/10")
    print(f"   Relevance: {result.get('relevance')}/10")
    print(f"   Completeness: {result.get('completeness')}/10")
    print(f"   Accuracy: {result.get('accuracy')}/10")
    print(f"\n   Justification: {result.get('justification')}")


if __name__ == "__main__":
    main()
